{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255d7c5d-5c27-4d6e-846e-59b169ca6195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Ans.\n",
    "\n",
    "R-squared (R²) is a statistical measure used to evaluate the goodness of fit of a linear regression model. \n",
    "It represents the proportion of the variance in the dependent variable (Y) that is explained by the independent\n",
    "variable (X) in the model.\n",
    "\n",
    "The R-squared value ranges from 0 to 1, where a value of 0 indicates that the model does not explain any of\n",
    "the variation in the dependent variable, and a value of 1 indicates that the model explains all of the variation\n",
    "in the dependent variable.\n",
    "\n",
    "To calculate R-squared, you first calculate the total sum of squares (TSS), which is the sum of the squared\n",
    "differences between each actual Y value and the mean Y value. Then, you calculate the\n",
    "residual sum of squares (RSS), which is the sum of the squared differences between each actual Y value\n",
    "and the predicted Y value from the model. Finally, you calculate R-squared by subtracting the RSS from \n",
    "the TSS and dividing the result by the TSS:\n",
    "\n",
    "R² = 1 - (RSS/TSS)\n",
    "\n",
    "R-squared is commonly used to assess the fit of linear regression models. A high R-squared value indicates a\n",
    "good fit between the model and the data, meaning that a high proportion of the variation in the dependent \n",
    "variable is explained by the independent variable. However, it is important to note that a high R-squared value\n",
    "does not necessarily mean that the model is a good predictor of future observations or that the independent\n",
    "variable causes the dependent variable. Additionally, R-squared should not be used as the sole criterion\n",
    "for selecting a model or for drawing conclusions about causality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c74227-102b-4b21-92f2-c820fdc293b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Ans.\n",
    "\n",
    "R-squared (R2) is a statistical measure that represents the proportion of variance in the dependent variable \n",
    "that is explained by the independent variables in a regression model. However, regular R-squared does not\n",
    "take into account the number of independent variables included in the model.\n",
    "\n",
    "Adjusted R-squared, on the other hand, is a modified version of R-squared that adjusts for the number of\n",
    "independent variables in the model. It takes into account the degree of freedom and the number of observations\n",
    "used in the model to give a more accurate estimate of the true population R-squared value.\n",
    "\n",
    "Adjusted R-squared is calculated as follows:\n",
    "\n",
    "Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - k - 1)]\n",
    "\n",
    "Where:\n",
    "\n",
    "R-squared: regular R-squared\n",
    "n: the number of observations used in the model\n",
    "k: the number of independent variables in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e836756-ac0a-4448-8d32-6749072e96d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Ans.\n",
    "\n",
    "Adjusted R-squared should be used when evaluating regression models that have more than one \n",
    "independent variable. Regular R-squared is not appropriate in this case because it does not take into account\n",
    "the number of independent variables in the model. Adjusted R-squared provides a better estimate of the true \n",
    "population R-squared value by adjusting for the number of independent variables in the model.\n",
    "\n",
    "In general, adjusted R-squared should be used when comparing models with different numbers of independent\n",
    "variables. When comparing models with the same number of independent variables, regular R-squared can be used\n",
    "as a measure of goodness-of-fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed9862-b7da-429d-804b-75f35c1fd36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Ans.\n",
    "\n",
    "RMSE, MSE, and MAE are commonly used metrics to evaluate the performance of a regression model.\n",
    "\n",
    "RMSE: Root Mean Squared Error\n",
    "RMSE is the square root of the mean of the squared differences between the predicted and actual values\n",
    "of the dependent variable. It is calculated as follows:\n",
    "\n",
    "RMSE = sqrt[ (1/n) * ∑(i=1 to n) (yi - ŷi)^2 ]\n",
    "\n",
    "Where:\n",
    "\n",
    "n: the number of observations\n",
    "yi: the actual value of the dependent variable for the i-th observation\n",
    "ŷi: the predicted value of the dependent variable for the i-th observation\n",
    "RMSE represents the average magnitude of the error between the predicted and actual values of the \n",
    "dependent variable, with a smaller value indicating better performance.\n",
    "\n",
    "MSE: Mean Squared Error\n",
    "MSE is the average of the squared differences between the predicted and actual values of the dependent variable.\n",
    "It is calculated as follows:\n",
    "\n",
    "MSE = (1/n) * ∑(i=1 to n) (yi - ŷi)^2\n",
    "\n",
    "Where:\n",
    "\n",
    "n, yi, and ŷi are the same as for RMSE\n",
    "MSE also represents the average magnitude of the error between the predicted and actual values of the dependent\n",
    "variable. However, unlike RMSE, it is not in the same unit as the dependent variable and may be harder to\n",
    "interpret.\n",
    "\n",
    "MAE: Mean Absolute Error\n",
    "MAE is the average of the absolute differences between the predicted and actual values of the dependent variable.\n",
    "It is calculated as follows:\n",
    "\n",
    "MAE = (1/n) * ∑(i=1 to n) |yi - ŷi|\n",
    "\n",
    "Where:\n",
    "\n",
    "n, yi, and ŷi are the same as for RMSE\n",
    "MAE represents the average absolute magnitude of the error between the predicted and actual values of the\n",
    "dependent variable. It is in the same unit as the dependent variable and is easier to interpret than MSE.\n",
    "\n",
    "In summary, RMSE, MSE, and MAE are all measures of the error between the predicted and actual values of the \n",
    "dependent variable. RMSE and MSE give more weight to larger errors, while MAE treats all errors equally.\n",
    "The choice of which metric to use depends on the context and the specific needs of the analysi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebfc6d3-02b9-423a-8f20-e26f02d48b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.Ans.\n",
    "\n",
    "RMSE, MSE, and MAE are commonly used evaluation metrics in regression analysis. \n",
    "Each metric has its own advantages and disadvantages, which are discussed below:\n",
    "\n",
    "Advantages of RMSE:\n",
    "\n",
    "RMSE penalizes larger errors more than smaller errors, which can be useful when the magnitude of the error\n",
    "is important.\n",
    "RMSE is a good metric to use when the distribution of errors is normal, as it assumes normality of errors.\n",
    "Disadvantages of RMSE:\n",
    "\n",
    "RMSE is sensitive to outliers, as larger errors will be more heavily weighted.\n",
    "RMSE is more difficult to interpret than MAE, as it is in the square root of the unit of the dependent variable.\n",
    "Advantages of MSE:\n",
    "\n",
    "MSE is a popular metric that is widely used in regression analysis because it is easy to compute.\n",
    "MSE is sensitive to the magnitude of errors, as it squares each error term.\n",
    "Disadvantages of MSE:\n",
    "\n",
    "Like RMSE, MSE is sensitive to outliers and may overemphasize the importance of large errors.\n",
    "MSE is in the square of the unit of the dependent variable, which makes it harder to interpret than MAE.\n",
    "Advantages of MAE:\n",
    "\n",
    "MAE treats all errors equally and is less sensitive to outliers than RMSE and MSE.\n",
    "MAE is in the same unit as the dependent variable, which makes it easy to interpret.\n",
    "Disadvantages of MAE:\n",
    "\n",
    "MAE does not differentiate between the magnitude of errors, which can be problematic if the magnitude \n",
    "of the error is important.\n",
    "MAE can be less sensitive to changes in the model than RMSE and MSE, which may make it less useful in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dae803-9681-42fe-b695-838136abc9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.Ans.\n",
    "\n",
    "Lasso regularization is a technique used in machine learning and statistical modeling to\n",
    "prevent overfitting by adding a penalty term to the cost function of the model. \n",
    "The penalty term is proportional to the absolute value of the coefficients, which encourages sparsity \n",
    "in the model by shrinking some of the coefficients towards zero. The name \"lasso\" is an acronym \n",
    "for \"Least Absolute Shrinkage and Selection Operator.\"\n",
    "\n",
    "Lasso regularization is similar to Ridge regularization, which also adds a penalty term to the \n",
    "cost function, but the penalty term in Ridge regularization is proportional to the square of the coefficients.\n",
    "This difference leads to different properties of the two techniques. Lasso regularization tends to produce\n",
    "sparse models, where many of the coefficients are exactly zero, while Ridge regularization typically shrinks\n",
    "all the coefficients towards zero but does not set any of them exactly to zero.\n",
    "\n",
    "The choice between Lasso and Ridge regularization depends on the specific problem and the characteristics\n",
    "of the data. If the data has many features, and it is suspected that only a small subset of them are important\n",
    "for the prediction task, then Lasso regularization may be more appropriate because it can effectively perform\n",
    "feature selection. On the other hand, if all the features are believed to be relevant, but the model is at\n",
    "risk of overfitting, Ridge regularization may be more appropriate because it will shrink all the coefficients\n",
    "towards zero without completely eliminating any of them. In general, it is a good practice to try both\n",
    "techniques and compare their performance using a validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1341cd2-d64d-4901-8ea5-7bf98dbf1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.Ans.\n",
    "\n",
    "Regularized linear models are a class of machine learning algorithms that add a penalty term to\n",
    "the cost function, which helps to prevent overfitting by limiting the model's complexity. In particular, \n",
    "the regularization term constrains the values of the model's coefficients, making them smaller and less \n",
    "sensitive to noise in the training data.\n",
    "\n",
    "For example, consider the problem of predicting housing prices based on features such as the number of rooms,\n",
    "the location, and the age of the house. A linear regression model can be used to make these predictions, \n",
    "but if the model is too complex, it may fit the training data too closely, leading to overfitting. \n",
    "To prevent overfitting, we can use a regularized linear model such as ridge regression or Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1fb253-38b3-4991-ae5d-c4d6c92cbc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.Ans.\n",
    "\n",
    "One limitation of regularized linear models is that they assume a linear relationship between the \n",
    "input features and the output variable. In some cases, the relationship between the features and \n",
    "the output may be nonlinear, and a linear model may not capture this relationship effectively. \n",
    "In such cases, more complex models such as decision trees or neural networks may be more appropriate.\n",
    "\n",
    "Finally, regularized linear models may not be suitable for datasets with a large number of features,\n",
    "as the computational cost of fitting the model and selecting the optimal regularization parameter can\n",
    "become prohibitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad3a71b-4a7d-418f-ab5b-9f3fb36d322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.Ans.\n",
    "\n",
    "To choose the better performing model, we need to look at both the RMSE and MAE values in the context of\n",
    "the specific problem and dataset.\n",
    "\n",
    "The RMSE measures the average magnitude of the errors in the predictions, with larger errors being penalized\n",
    "more heavily. A smaller RMSE indicates better predictive accuracy and precision. In this case, Model A has an\n",
    "RMSE of 10, which means that, on average, the predictions of Model A are off by 10 units.\n",
    "\n",
    "The MAE, on the other hand, measures the average absolute magnitude of the errors, with all errors being treated\n",
    "equally. A smaller MAE indicates better accuracy but does not differentiate between large and small errors. \n",
    "In this case, Model B has an MAE of 8, which means that, on average, the predictions of Model B are off by 8 units.\n",
    "\n",
    "Based solely on these metrics, we might be inclined to choose Model B as the better performer, since it has\n",
    "a lower MAE value. However, it is important to note that the choice of metric depends on the specific problem\n",
    "and the specific goals of the analysis.\n",
    "\n",
    "For example, if the goal of the analysis is to minimize the impact of large errors, then the RMSE might be a\n",
    "more appropriate metric. On the other hand, if the goal is to minimize the overall error, regardless of the\n",
    "magnitude, then the MAE might be more appropriate.\n",
    "\n",
    "Another limitation of these metrics is that they do not take into account the context of the problem or the\n",
    "cost of different types of errors. For example, in some applications, such as medical diagnosis, \n",
    "false negatives (i.e., failing to predict a positive outcome) may be more costly than false positives. \n",
    "In such cases, other metrics, such as the F1 score or the ROC curve, may be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f50ca8-79f9-4d8b-b632-1478decf3dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.Ans.\n",
    "\n",
    "To compare the performance of two regularized linear models using different types of regularization,\n",
    "we need to evaluate them based on a suitable metric, such as mean squared error (MSE) or R-squared.\n",
    "\n",
    "Assuming we have chosen an appropriate metric and evaluated the models, we can compare their performance\n",
    "and choose the better performer. However, the choice of regularization method depends on the specific problem \n",
    "and the characteristics of the dataset.\n",
    "\n",
    "In general, Ridge regularization (L2 regularization) is more suitable when there are many small coefficients, \n",
    "while Lasso regularization (L1 regularization) is more suitable when there are only a few important coefficients.\n",
    "Ridge regularization tends to shrink the coefficients towards zero, but it does not set any of them exactly to\n",
    "zero, while Lasso regularization can set some coefficients exactly to zero, resulting in a more interpretable\n",
    "and sparse model.\n",
    "\n",
    "In this specific case, we have Model A using Ridge regularization with a regularization parameter of 0.1, and\n",
    "Model B using Lasso regularization with a regularization parameter of 0.5. Based on this limited information,\n",
    "we cannot determine which model is the better performer without further evaluation.\n",
    "\n",
    "However, we can discuss the trade-offs and limitations of the two regularization methods. \n",
    "Ridge regularization can be less effective than Lasso regularization in reducing the number of features in the\n",
    "model. In addition, the choice of the regularization parameter can be less critical for Ridge regularization, \n",
    "as it has a less drastic effect on the coefficients. Lasso regularization can be more effective in feature \n",
    "selection and can produce a more interpretable model, but the choice of the regularization parameter can be \n",
    "more critical, as it can lead to overfitting or underfitting.\n",
    "\n",
    "In summary, the choice of regularization method depends on the specific problem and the characteristics of the \n",
    "dataset. Both Ridge and Lasso regularization have their trade-offs and limitations, and the choice between\n",
    "them should be made based on the specific goals of the analysis and the characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4b5ad2-653c-4c67-b36a-4b05c2000ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
